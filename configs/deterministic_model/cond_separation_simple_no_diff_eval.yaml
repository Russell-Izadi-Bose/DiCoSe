# @package _global_

project_name: "Deterministic_model"
log_directory: "lightning_logs"
mode: test # train | test | validate_all

id:
  version: "Testing_audiodm_cond_separation_simple"
  name: ""

model:
  _target_: train_audio_simple.Audio_DM_Model_simple
  learning_rate: 1.e-4
  beta1: 0.9
  beta2: 0.99
  class_cond: True
  separation: False #True

  # these are if you call conditional and serve classifier free guidance purpose
  # embedding_features: 768       # this gets passed to -> context_embedding_features
  # embedding_max_length: 64     # this gets passed to -> context_embedding_max_length
  # embedding_mask_proba: 0.1

  context_features: 4 # ???
  context_channels: null #[1, 512, 1024, 1024, 1024, 1024, 1024] ### Optional[Sequence[int]] = None, this decides use_context_channels (might be somethig usefull)
  # context_embedding_features: 32 #Optional[int] = None,

  in_channels: 1
  channels: 256
  patch_factor: 16
  patch_blocks: 1
  resnet_groups: 8
  kernel_multiplier_downsample: 2
  kernel_sizes_init: [1, 3, 7]
  multipliers: [1, 2, 4, 4, 4, 4, 4]
  factors: [4, 4, 4, 2, 2, 2]
  num_blocks: [2, 2, 2, 2, 2, 2]
  attentions: [False, False, False, True, True, True]
  attention_heads: 8
  attention_features: 128
  attention_multiplier: 2
  use_nearest_upsample: False
  use_skip_scale: True
  use_attention_bottleneck: True
  use_context_time: False




train_dataset:
  _target_: main.data.TracksDataset
  audio_files_dir: "dataset/slakh2100/train"
  sr: 22050
  channels: 1
  min_duration: 12.0
  max_duration: 640.0
  aug_shift: True
  sample_length: 262144
  stems: ['bass', 'drums', 'guitar', 'piano']

# val_dataset:
#   _target_: main.data.TracksDataset
#   audio_files_dir: "dataset/slakh2100/validation"
#   sr: 22050
#   channels: 1
#   min_duration: 12.0
#   max_duration: 640.0
#   aug_shift: False #True
#   sample_length: 262144
#   stems: ['bass', 'drums', 'guitar', 'piano']


val_dataset:
  _target_: main.data.ChunkedSupervisedDataset_for_extraction
  audio_dir: "dataset/slakh2100/test"
  sample_rate: 22050
  max_chunk_size: 262144
  min_chunk_size: 262144
  stems: ['bass', 'drums', 'guitar', 'piano']
  only_multisource: True


datamodule:
  batch_size: 50 #8
  num_workers: 4
  pin_memory: True



audio_samples_logger:
  _target_: train_audio_simple.ClassCondSeparateTrackSampleLogger_simple
  num_items: 2
  channels: 1
  sampling_rate: 22050
  length: 262144
  stems: ['bass', 'drums', 'guitar', 'piano']



callbacks:
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar

  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: 1




trainer:
  accelerator: cuda
  devices: [0] # 1, 2, 3] # Set `1` to train on GPU, `0` to train on CPU only, and `-1` to train on all GPUs, default `0`
  precision: "bf16" # Precision used for tensors, default `32`
  min_epochs: 0
  max_epochs: -1
  enable_model_summary: False
  log_every_n_steps: 10 # Logs metrics every N batches
  check_val_every_n_epoch: null
  val_check_interval: #4000
  limit_val_batches: 5
  limit_train_batches:
  strategy: ddp
  num_sanity_val_steps: 0
  

resume_from_checkpoint: #lightning_logs/Deterministic_model/2024-08-06T00-20-47__Testing_audiodm_cond_separation_simple/checkpoints/last.ckpt