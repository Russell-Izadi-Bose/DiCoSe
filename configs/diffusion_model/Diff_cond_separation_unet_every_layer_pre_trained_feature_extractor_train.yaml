# @package _global_

project_name: "diffusion_model"
log_directory: "lightning_logs"
mode: train # train | test | validate_all

id:
  version: "Diffusion_Audiodm_cond_separation_with_pre-trained_features"
  name: ""

model:
  _target_: main.module_base.Audio_DM_Model
  learning_rate: 1.e-4
  beta1: 0.9
  beta2: 0.99
  class_cond: True
  separation: True

  # these are if you call conditional and serve classifier free guidance purpose
  # embedding_features: 768       # this gets passed to -> context_embedding_features
  # embedding_max_length: 64     # this gets passed to -> context_embedding_max_length
  # embedding_mask_proba: 0.1

  context_features: 4 # ???
  # context_channels: [1, 512, 1024, 1024, 1024, 1024, 1024,  1024, 1024, 1024, 1024, 1024, 1024, 512, 1] ### Optional[Sequence[int]] = None, this decides use_context_channels (might be somethig usefull)
  # context_embedding_features: 32 #Optional[int] = None,
  # mixture_features_channels: [1, 256, 512, 1024, 1024, 1024, 1024, 1024,  1024, 1024, 1024, 1024, 1024, 512, 256]
  pre_trained_mixture_feature_extractor: lightning_logs/MSDM/2024-08-06T00-20-47__Testing_audiodm_cond_separation_simple/checkpoints/last.ckpt

  in_channels: 1
  channels: 256
  patch_factor: 16
  patch_blocks: 1
  resnet_groups: 8
  kernel_multiplier_downsample: 2
  kernel_sizes_init: [1, 3, 7]
  multipliers: [1, 2, 4, 4, 4, 4, 4]
  factors: [4, 4, 4, 2, 2, 2]
  num_blocks: [2, 2, 2, 2, 2, 2]
  attentions: [False, False, False, True, True, True]
  attention_heads: 8
  attention_features: 128
  attention_multiplier: 2
  use_nearest_upsample: False
  use_skip_scale: True
  use_attention_bottleneck: True
  diffusion_sigma_data: 0.2
  diffusion_dynamic_threshold: 0.0

diffusion_sigma_distribution:
  _target_: audio_diffusion_pytorch_.LogNormalDistribution
  mean: -3.0
  std: 1.0



train_dataset:
  _target_: main.data.TracksDataset
  audio_files_dir: "dataset/slakh2100/train"
  sr: 22050
  channels: 1
  min_duration: 12.0
  max_duration: 640.0
  aug_shift: True
  sample_length: 262144
  stems: ['bass', 'drums', 'guitar', 'piano']

val_dataset:
  _target_: main.data.TracksDataset
  audio_files_dir: "dataset/slakh2100/validation"
  sr: 22050
  channels: 1
  min_duration: 12.0
  max_duration: 640.0
  aug_shift: False #True
  sample_length: 262144
  stems: ['bass', 'drums', 'guitar', 'piano']

datamodule:
  batch_size: 50 #8
  num_workers: 4
  pin_memory: True



audio_samples_logger:
  _target_: main.module_base.ClassCondSeparateTrackSampleLogger
  num_items: 2
  channels: 1
  sampling_rate: 22050
  length: 262144
  sampling_steps: 100
  stems: ['bass', 'drums', 'guitar', 'piano']
diffusion_sampler:
  _target_: audio_diffusion_pytorch_.ADPM2Sampler
  rho: 1.0
diffusion_schedule:
  _target_: audio_diffusion_pytorch_.KarrasSchedule
  sigma_min: 0.0001
  sigma_max: 3.0
  rho: 9.0



callbacks:
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar

  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: 1




trainer:
  accelerator: cuda
  devices: [0] #, 1, 2, 3] # Set `1` to train on GPU, `0` to train on CPU only, and `-1` to train on all GPUs, default `0`
  precision: "bf16" # Precision used for tensors, default `32`
  min_epochs: 0
  max_epochs: -1
  enable_model_summary: False
  log_every_n_steps: 10 # Logs metrics every N batches
  check_val_every_n_epoch: 20 # null
  val_check_interval: #4000
  limit_val_batches: 6
  limit_train_batches: 2
  strategy: ddp
  num_sanity_val_steps: 0
  

resume_from_checkpoint: